{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train = 'data/snli_train.tsv'\n",
    "val = 'data/snli_val.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_matrix():\n",
    "    #load fasttext word vectors\n",
    "    words_to_load = 50000\n",
    "\n",
    "    with open('wiki-news-300d-1M-subword.vec') as f:\n",
    "        #remove the first line\n",
    "        firstLine = f.readline()\n",
    "        loaded_embeddings = np.zeros((words_to_load + 2, 300))\n",
    "        words2id = {}\n",
    "        idx2words = {}\n",
    "        #ordered_words = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= words_to_load: \n",
    "                break\n",
    "            s = line.split()\n",
    "            loaded_embeddings[i + 2 , :] = np.asarray(s[1:])\n",
    "            words2id['<pad>'] = PAD_IDX\n",
    "            words2id['<unk>'] = UNK_IDX\n",
    "            words2id[s[0]] = i + 2\n",
    "            idx2words[i + 2] = s[0]\n",
    "            idx2words[0] = '<pad>'\n",
    "            idx2words[1] = '<unk>'\n",
    "\n",
    "    return words2id,idx2words,loaded_embeddings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2id,idx2words,loaded_embeddings = load_emb_matrix()\n",
    "\n",
    "# pkl.dump(words2id, open(f'data/words2id.pkl', 'wb'))\n",
    "# pkl.dump(idx2words, open(f'data/idx2words.pkl', 'wb'))\n",
    "# pkl.dump(loaded_embeddings, open(f'data/embedding_matrix.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens \n",
    "            if (token.text not in punctuations) & (token.text not in STOP_WORDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    #all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        #all_tokens += tokens\n",
    "\n",
    "    return token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words2id[word] if word in words2id else UNK_IDX for word in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train,delimiter='\\t',encoding='utf-8')\n",
    "val_data = pd.read_csv(val,delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "train_data['label_num'] = train_data['label'].apply(lambda x: 0 if str(x) == 'contradiction' else 1 if str(x) == 'neutral' else 2)\n",
    "val_data['label_num'] = val_data['label'].apply(lambda x: 0 if str(x) == 'contradiction' else 1 if str(x) == 'neutral' else 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokens_1 = tokenize_dataset(train_data['sentence1'])\n",
    "# train_tokens_2 = tokenize_dataset(train_data['sentence2'])\n",
    "# pkl.dump(train_tokens_1, open(\"data/train_data_tokens_1.p\", \"wb\"))\n",
    "# pkl.dump(train_tokens_2, open(\"data/train_data_tokens_2.p\", \"wb\"))\n",
    "\n",
    "# val_tokens_1 = tokenize_dataset(val_data['sentence1'])\n",
    "# val_tokens_2 = tokenize_dataset(val_data['sentence2'])\n",
    "# pkl.dump(val_tokens_1, open(\"data/val_data_tokens_1.p\", \"wb\"))\n",
    "# pkl.dump(val_tokens_2, open(\"data/val_data_tokens_2.p\", \"wb\"))\n",
    "\n",
    "train_tokens_1 = pkl.load(open(\"data/train_data_tokens_1.p\", \"rb\"))\n",
    "train_tokens_2 = pkl.load(open(\"data/train_data_tokens_2.p\", \"rb\"))\n",
    "\n",
    "val_tokens_1 = pkl.load(open(\"data/val_data_tokens_1.p\", \"rb\"))\n",
    "val_tokens_2 = pkl.load(open(\"data/val_data_tokens_2.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indices_1 = token2index_dataset(train_tokens_1)\n",
    "train_data_indices_2 = token2index_dataset(train_tokens_2)\n",
    "val_data_indices_1 = token2index_dataset(val_tokens_1)\n",
    "val_data_indices_2 = token2index_dataset(val_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = list(train_data['label_num'])\n",
    "val_label = list(val_data['label_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 45\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_list_1, data_list_2, target_list, words2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_1 = data_list_1\n",
    "        self.data_2 = data_list_2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_1) == len(self.target_list))\n",
    "        assert (len(self.data_2) == len(self.target_list))\n",
    "        self.words2id = words2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        words_idx_1 = self.data_1[key][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        words_idx_2 = self.data_2[key][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        label = self.target_list[key]\n",
    "        return [words_idx_1, len(words_idx_1), words_idx_2,len(words_idx_2),label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_1 = []\n",
    "    data_list_2 = []\n",
    "    label_list = []\n",
    "    length_list_1 = []\n",
    "    length_list_2 = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list_1.append(datum[1])\n",
    "        length_list_2.append(datum[3])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_1.append(padded_vec_1)\n",
    "        \n",
    "        padded_vec_2 = np.pad(np.array(datum[2]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_2.append(padded_vec_2)\n",
    "        \n",
    "    ind_dec_order_1 = np.argsort(length_list_1)[::-1]\n",
    "    ind_dec_order_2 = np.argsort(length_list_2)[::-1]\n",
    "    \n",
    "    data_list_1 = np.array(data_list_1)[ind_dec_order_1]\n",
    "    length_list_1 = np.array(length_list_1)[ind_dec_order_1]\n",
    "    \n",
    "    data_list_2 = np.array(data_list_2)[ind_dec_order_2]\n",
    "    length_list_2 = np.array(length_list_2)[ind_dec_order_2]\n",
    "    #handle torch type problem by adding the following line\n",
    "    data_list_2 = np.asarray(data_list_2, dtype=int)\n",
    "    label_list = np.array(label_list)[ind_dec_order_1]\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list_1)), torch.LongTensor(length_list_1), \n",
    "            torch.from_numpy(np.array(data_list_2)), torch.LongTensor(length_list_2),\n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "\n",
    "train_dataset = VocabDataset(train_data_indices_1, train_data_indices_2, train_label,words2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = VocabDataset(val_data_indices_1,val_data_indices_2,val_label, words2id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights_matrix(train_data_indices_1,train_data_indices_2,loaded_embeddings):\n",
    "    all_tokens = []\n",
    "    for line in train_data_indices_1:\n",
    "        for token in line:\n",
    "            all_tokens.append(token)\n",
    "\n",
    "    for line in train_data_indices_2:\n",
    "        for token in line:\n",
    "            all_tokens.append(token)\n",
    "            \n",
    "    matrix_len = len(all_tokens)\n",
    "    weights_matrix = np.zeros((matrix_len, 300))\n",
    "    words_found = 0\n",
    "    \n",
    "    for i, idx in enumerate(all_tokens):\n",
    "        try: \n",
    "            weights_matrix[i] = loaded_embeddings[idx]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = generate_weights_matrix(train_data_indices_1,train_data_indices_2,loaded_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-557172f9526e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# RNN Accepts the following hyperparams:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# hidden_size: Hidden Size of layer in RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# num_layers: number of layers in RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.bi_gru = nn.GRU(embedding_dim, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear = nn.Linear(2 * hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        #biased term?\n",
    "        hidden = torch.randn(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, x, lengths_x, y,lengths_y):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size_x, seq_len_x = x.size()\n",
    "        self.hidden_x = self.init_hidden(batch_size_x)\n",
    "        \n",
    "        # get embedding of characters\n",
    "        embed_x = self.embedding(x)        \n",
    "        rnn_out_x, _ = self.bi_gru(embed_x,self.hidden_x)\n",
    "        rnn_out_x = rnn_out_x[:, -1, :self.hidden_size] + rnn_out_x[:, 0, self.hidden_size:] \n",
    "\n",
    "        \n",
    "        batch_size_y, seq_len_y = y.size()\n",
    "        self.hidden_y = self.init_hidden(batch_size_y)\n",
    "        \n",
    "        embed_y = self.embedding(y)           \n",
    "        rnn_out_y, self.hidden_y = self.bi_gru(embed_y, self.hidden_y)\n",
    "        #rnn_out_y = rnn_out_y[:, -1, :self.hidden_size] + rnn_out_y[0, :, self.hidden_size:]\n",
    "        \n",
    "        #run_out = torch.cat([rnn_out_x, rnn_out_y], 1)\n",
    "        \n",
    "#         logits = self.linear(rnn_out)\n",
    "        \n",
    "        return rnn_out_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_1, lengths_1, data_2, lengths_2, labels in loader:\n",
    "        data_batch_1, lengths_batch_1, data_batch_2, lengths_batch_2, label_batch = data_1, lengths_1, data_2, lengths_2,labels\n",
    "        outputs = F.softmax(model(data_batch_1, lengths_batch_1,data_batch_2, lengths_batch_2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(weights_matrix, hidden_size=200, num_layers=1, num_classes = 3, vocab_size=len(idx2words))\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data_1, lengths_1, data_2, lengths_2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data_1, lengths_1,data_2,lengths_2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (data_1, lengths_1, data_2, lengths_2, labels) in enumerate(train_loader):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(data_1, lengths_1,data_2,lengths_2)\n",
    "    #loss = criterion(outputs, labels)\n",
    "    \n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 45, 400])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
