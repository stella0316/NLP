{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "random.seed(134)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train = 'data/snli_train.tsv'\n",
    "val = 'data/snli_val.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words2id[word] if word in words2id else UNK_IDX for word in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2id = pkl.load(open(\"data/words2id.pkl\", \"rb\"))\n",
    "idx2words = pkl.load(open(\"data/idx2words.pkl\", \"rb\"))\n",
    "loaded_embeddings = pkl.load(open(\"data/embedding_matrix.pkl\", \"rb\"))\n",
    "\n",
    "train_data = pd.read_csv(train,delimiter='\\t',encoding='utf-8')\n",
    "val_data = pd.read_csv(val,delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "train_data['label_num'] = train_data['label'].apply(lambda x: 0 if str(x) == 'contradiction' else 1 if str(x) == 'neutral' else 2)\n",
    "val_data['label_num'] = val_data['label'].apply(lambda x: 0 if str(x) == 'contradiction' else 1 if str(x) == 'neutral' else 2)\n",
    "\n",
    "train_label = list(train_data['label_num'])\n",
    "val_label = list(val_data['label_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_1 = pkl.load(open(\"data/train_data_tokens_1.p\", \"rb\"))\n",
    "train_tokens_2 = pkl.load(open(\"data/train_data_tokens_2.p\", \"rb\"))\n",
    "\n",
    "val_tokens_1 = pkl.load(open(\"data/val_data_tokens_1.p\", \"rb\"))\n",
    "val_tokens_2 = pkl.load(open(\"data/val_data_tokens_2.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indices_1 = token2index_dataset(train_tokens_1)\n",
    "train_data_indices_2 = token2index_dataset(train_tokens_2)\n",
    "val_data_indices_1 = token2index_dataset(val_tokens_1)\n",
    "val_data_indices_2 = token2index_dataset(val_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 45\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_list_1, data_list_2, target_list, words2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_1 = data_list_1\n",
    "        self.data_2 = data_list_2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_1) == len(self.target_list))\n",
    "        assert (len(self.data_2) == len(self.target_list))\n",
    "        self.words2id = words2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        words_idx_1 = self.data_1[key][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        words_idx_2 = self.data_2[key][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        label = self.target_list[key]\n",
    "        \n",
    "        return [words_idx_1, len(words_idx_1), words_idx_2,len(words_idx_2),label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_1 = []\n",
    "    data_list_2 = []\n",
    "    label_list = []\n",
    "    length_list_1 = []\n",
    "    length_list_2 = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list_1.append(datum[1])\n",
    "        length_list_2.append(datum[3])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_1.append(padded_vec_1)\n",
    "        \n",
    "        padded_vec_2 = np.pad(np.array(datum[2]),\n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_2.append(padded_vec_2)\n",
    "        \n",
    "\n",
    "\n",
    "    #handle torch type problem by adding the following line\n",
    "    data_list_2 = np.asarray(data_list_2, dtype=int)\n",
    "    label_list = np.array(label_list)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list_1)), torch.LongTensor(length_list_1), \n",
    "            torch.from_numpy(np.array(data_list_2)), torch.LongTensor(length_list_2),\n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train and valid dataloaders\n",
    "\n",
    "train_dataset = VocabDataset(train_data_indices_1, train_data_indices_2, train_label,words2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "val_dataset = VocabDataset(val_data_indices_1,val_data_indices_2,val_label, words2id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weights_matrix(idx2words,loaded_embeddings):\n",
    "   \n",
    "    matrix_len = len(idx2words)\n",
    "    weights_matrix = np.zeros((matrix_len, 300))\n",
    "    \n",
    "    for key in idx2words.keys():\n",
    "        try: \n",
    "            weights_matrix[key] = loaded_embeddings[key]\n",
    "        except KeyError:\n",
    "            weights_matrix[key] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = generate_weights_matrix(idx2words,loaded_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, num_classes,kernel_size,stride):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim,padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, hidden_size, kernel_size=5, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, hidden_size, kernel_size=5, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(4200, 180)\n",
    "        self.fc2 = nn.Linear(180, num_classes)\n",
    "        \n",
    " \n",
    "    \n",
    "    def forward(self, x,  y):\n",
    "        embed_x = self.embedding(x)\n",
    "        hidden_x = self.layer1(embed_x.transpose(1,2))\n",
    "        \n",
    "        embed_y = self.embedding(y)\n",
    "        hidden_y = self.layer2(embed_y.transpose(1,2))\n",
    "        \n",
    "        hidden = torch.cat([hidden_x, hidden_y], 1)\n",
    "\n",
    "        out = hidden.reshape(hidden.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out.contiguous().view(-1, out.size(-1)))\n",
    "        \n",
    "        logits = self.fc2(out)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    model.eval()\n",
    "    for data_1,lengths_1, data_2, lengths_2,labels in loader:\n",
    "        data_batch_1, data_batch_2, label_batch = data_1, data_2, labels\n",
    "        outputs = F.softmax(model(data_batch_1, data_batch_2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss_val += loss.item() * len(data) / len(loader.dataset)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).double().sum().item()\n",
    "    return (100 * correct / total), loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/3125], Validation Acc: 38.9\n",
      "Epoch: [1/10], Step: [201/3125], Validation Acc: 39.9\n",
      "Epoch: [1/10], Step: [301/3125], Validation Acc: 42.3\n",
      "Epoch: [1/10], Step: [401/3125], Validation Acc: 44.9\n",
      "Epoch: [1/10], Step: [501/3125], Validation Acc: 47.9\n",
      "Epoch: [1/10], Step: [601/3125], Validation Acc: 48.5\n",
      "Epoch: [1/10], Step: [701/3125], Validation Acc: 49.4\n",
      "Epoch: [1/10], Step: [801/3125], Validation Acc: 49.8\n",
      "Epoch: [1/10], Step: [901/3125], Validation Acc: 50.6\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 52.6\n",
      "Epoch: [1/10], Step: [1101/3125], Validation Acc: 53.6\n",
      "Epoch: [1/10], Step: [1201/3125], Validation Acc: 54.2\n",
      "Epoch: [1/10], Step: [1301/3125], Validation Acc: 54.2\n",
      "Epoch: [1/10], Step: [1401/3125], Validation Acc: 53.7\n",
      "Epoch: [1/10], Step: [1501/3125], Validation Acc: 54.9\n",
      "Epoch: [1/10], Step: [1601/3125], Validation Acc: 53.1\n",
      "Epoch: [1/10], Step: [1701/3125], Validation Acc: 52.5\n",
      "Epoch: [1/10], Step: [1801/3125], Validation Acc: 54.0\n",
      "Epoch: [1/10], Step: [1901/3125], Validation Acc: 54.5\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 54.9\n",
      "Epoch: [1/10], Step: [2101/3125], Validation Acc: 55.4\n",
      "Epoch: [1/10], Step: [2201/3125], Validation Acc: 55.2\n",
      "Epoch: [1/10], Step: [2301/3125], Validation Acc: 55.1\n",
      "Epoch: [1/10], Step: [2401/3125], Validation Acc: 56.1\n",
      "Epoch: [1/10], Step: [2501/3125], Validation Acc: 55.7\n",
      "Epoch: [1/10], Step: [2601/3125], Validation Acc: 56.7\n",
      "Epoch: [1/10], Step: [2701/3125], Validation Acc: 56.0\n",
      "Epoch: [1/10], Step: [2801/3125], Validation Acc: 56.8\n",
      "Epoch: [1/10], Step: [2901/3125], Validation Acc: 56.8\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 56.6\n",
      "Epoch: [1/10], Step: [3101/3125], Validation Acc: 56.0\n",
      "Epoch: [2/10], Step: [101/3125], Validation Acc: 55.7\n",
      "Epoch: [2/10], Step: [201/3125], Validation Acc: 56.7\n",
      "Epoch: [2/10], Step: [301/3125], Validation Acc: 57.9\n",
      "Epoch: [2/10], Step: [401/3125], Validation Acc: 57.3\n",
      "Epoch: [2/10], Step: [501/3125], Validation Acc: 58.7\n",
      "Epoch: [2/10], Step: [601/3125], Validation Acc: 55.9\n",
      "Epoch: [2/10], Step: [701/3125], Validation Acc: 58.6\n",
      "Epoch: [2/10], Step: [801/3125], Validation Acc: 58.0\n",
      "Epoch: [2/10], Step: [901/3125], Validation Acc: 57.0\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 58.0\n",
      "Epoch: [2/10], Step: [1101/3125], Validation Acc: 57.0\n",
      "Epoch: [2/10], Step: [1201/3125], Validation Acc: 58.6\n",
      "Epoch: [2/10], Step: [1301/3125], Validation Acc: 58.5\n",
      "Epoch: [2/10], Step: [1401/3125], Validation Acc: 59.2\n",
      "Epoch: [2/10], Step: [1501/3125], Validation Acc: 59.8\n",
      "Epoch: [2/10], Step: [1601/3125], Validation Acc: 58.2\n",
      "Epoch: [2/10], Step: [1701/3125], Validation Acc: 56.4\n",
      "Epoch: [2/10], Step: [1801/3125], Validation Acc: 58.5\n",
      "Epoch: [2/10], Step: [1901/3125], Validation Acc: 59.1\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 57.7\n",
      "Epoch: [2/10], Step: [2101/3125], Validation Acc: 58.5\n",
      "Epoch: [2/10], Step: [2201/3125], Validation Acc: 59.5\n",
      "Epoch: [2/10], Step: [2301/3125], Validation Acc: 58.1\n",
      "Epoch: [2/10], Step: [2401/3125], Validation Acc: 58.9\n",
      "Epoch: [2/10], Step: [2501/3125], Validation Acc: 58.2\n",
      "Epoch: [2/10], Step: [2601/3125], Validation Acc: 57.9\n",
      "Epoch: [2/10], Step: [2701/3125], Validation Acc: 59.4\n",
      "Epoch: [2/10], Step: [2801/3125], Validation Acc: 57.8\n",
      "Epoch: [2/10], Step: [2901/3125], Validation Acc: 58.5\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 58.8\n",
      "Epoch: [2/10], Step: [3101/3125], Validation Acc: 58.4\n",
      "Epoch: [3/10], Step: [101/3125], Validation Acc: 59.0\n",
      "Epoch: [3/10], Step: [201/3125], Validation Acc: 58.2\n",
      "Epoch: [3/10], Step: [301/3125], Validation Acc: 59.3\n",
      "Epoch: [3/10], Step: [401/3125], Validation Acc: 60.6\n",
      "Epoch: [3/10], Step: [501/3125], Validation Acc: 61.2\n",
      "Epoch: [3/10], Step: [601/3125], Validation Acc: 60.0\n",
      "Epoch: [3/10], Step: [701/3125], Validation Acc: 58.8\n",
      "Epoch: [3/10], Step: [801/3125], Validation Acc: 60.0\n",
      "Epoch: [3/10], Step: [901/3125], Validation Acc: 59.8\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 58.8\n",
      "Epoch: [3/10], Step: [1101/3125], Validation Acc: 60.0\n",
      "Epoch: [3/10], Step: [1201/3125], Validation Acc: 60.0\n",
      "Epoch: [3/10], Step: [1301/3125], Validation Acc: 60.7\n",
      "Epoch: [3/10], Step: [1401/3125], Validation Acc: 59.6\n",
      "Epoch: [3/10], Step: [1501/3125], Validation Acc: 60.9\n",
      "Epoch: [3/10], Step: [1601/3125], Validation Acc: 60.0\n",
      "Epoch: [3/10], Step: [1701/3125], Validation Acc: 60.1\n",
      "Epoch: [3/10], Step: [1801/3125], Validation Acc: 59.4\n",
      "Epoch: [3/10], Step: [1901/3125], Validation Acc: 59.3\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 61.5\n",
      "Epoch: [3/10], Step: [2101/3125], Validation Acc: 60.2\n",
      "Epoch: [3/10], Step: [2201/3125], Validation Acc: 59.4\n",
      "Epoch: [3/10], Step: [2301/3125], Validation Acc: 59.8\n",
      "Epoch: [3/10], Step: [2401/3125], Validation Acc: 60.7\n",
      "Epoch: [3/10], Step: [2501/3125], Validation Acc: 60.2\n",
      "Epoch: [3/10], Step: [2601/3125], Validation Acc: 61.3\n",
      "Epoch: [3/10], Step: [2701/3125], Validation Acc: 60.4\n",
      "Epoch: [3/10], Step: [2801/3125], Validation Acc: 60.5\n",
      "Epoch: [3/10], Step: [2901/3125], Validation Acc: 60.1\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 60.9\n",
      "Epoch: [3/10], Step: [3101/3125], Validation Acc: 60.8\n",
      "Epoch: [4/10], Step: [101/3125], Validation Acc: 58.7\n",
      "Epoch: [4/10], Step: [201/3125], Validation Acc: 60.0\n",
      "Epoch: [4/10], Step: [301/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [401/3125], Validation Acc: 60.7\n",
      "Epoch: [4/10], Step: [501/3125], Validation Acc: 60.8\n",
      "Epoch: [4/10], Step: [601/3125], Validation Acc: 60.5\n",
      "Epoch: [4/10], Step: [701/3125], Validation Acc: 60.1\n",
      "Epoch: [4/10], Step: [801/3125], Validation Acc: 60.8\n",
      "Epoch: [4/10], Step: [901/3125], Validation Acc: 59.7\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 60.4\n",
      "Epoch: [4/10], Step: [1101/3125], Validation Acc: 59.7\n",
      "Epoch: [4/10], Step: [1201/3125], Validation Acc: 60.8\n",
      "Epoch: [4/10], Step: [1301/3125], Validation Acc: 61.0\n",
      "Epoch: [4/10], Step: [1401/3125], Validation Acc: 61.3\n",
      "Epoch: [4/10], Step: [1501/3125], Validation Acc: 61.8\n",
      "Epoch: [4/10], Step: [1601/3125], Validation Acc: 61.2\n",
      "Epoch: [4/10], Step: [1701/3125], Validation Acc: 62.2\n",
      "Epoch: [4/10], Step: [1801/3125], Validation Acc: 59.7\n",
      "Epoch: [4/10], Step: [1901/3125], Validation Acc: 60.4\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 61.0\n",
      "Epoch: [4/10], Step: [2101/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [2201/3125], Validation Acc: 62.1\n",
      "Epoch: [4/10], Step: [2301/3125], Validation Acc: 60.5\n",
      "Epoch: [4/10], Step: [2401/3125], Validation Acc: 60.1\n",
      "Epoch: [4/10], Step: [2501/3125], Validation Acc: 61.5\n",
      "Epoch: [4/10], Step: [2601/3125], Validation Acc: 60.1\n",
      "Epoch: [4/10], Step: [2701/3125], Validation Acc: 61.4\n",
      "Epoch: [4/10], Step: [2801/3125], Validation Acc: 59.2\n",
      "Epoch: [4/10], Step: [2901/3125], Validation Acc: 61.1\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 62.0\n",
      "Epoch: [4/10], Step: [3101/3125], Validation Acc: 60.9\n",
      "Epoch: [5/10], Step: [101/3125], Validation Acc: 60.7\n",
      "Epoch: [5/10], Step: [201/3125], Validation Acc: 61.1\n",
      "Epoch: [5/10], Step: [301/3125], Validation Acc: 62.0\n",
      "Epoch: [5/10], Step: [401/3125], Validation Acc: 60.7\n",
      "Epoch: [5/10], Step: [501/3125], Validation Acc: 61.0\n",
      "Epoch: [5/10], Step: [601/3125], Validation Acc: 62.4\n",
      "Epoch: [5/10], Step: [701/3125], Validation Acc: 61.0\n",
      "Epoch: [5/10], Step: [801/3125], Validation Acc: 60.7\n",
      "Epoch: [5/10], Step: [901/3125], Validation Acc: 61.5\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 62.5\n",
      "Epoch: [5/10], Step: [1101/3125], Validation Acc: 61.7\n",
      "Epoch: [5/10], Step: [1201/3125], Validation Acc: 61.7\n",
      "Epoch: [5/10], Step: [1301/3125], Validation Acc: 60.5\n",
      "Epoch: [5/10], Step: [1401/3125], Validation Acc: 60.7\n",
      "Epoch: [5/10], Step: [1501/3125], Validation Acc: 58.8\n",
      "Epoch: [5/10], Step: [1601/3125], Validation Acc: 59.9\n",
      "Epoch: [5/10], Step: [1701/3125], Validation Acc: 60.6\n",
      "Epoch: [5/10], Step: [1801/3125], Validation Acc: 61.2\n",
      "Epoch: [5/10], Step: [1901/3125], Validation Acc: 61.4\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 61.0\n",
      "Epoch: [5/10], Step: [2101/3125], Validation Acc: 60.4\n",
      "Epoch: [5/10], Step: [2201/3125], Validation Acc: 61.2\n",
      "Epoch: [5/10], Step: [2301/3125], Validation Acc: 61.8\n",
      "Epoch: [5/10], Step: [2401/3125], Validation Acc: 60.8\n",
      "Epoch: [5/10], Step: [2501/3125], Validation Acc: 61.7\n",
      "Epoch: [5/10], Step: [2601/3125], Validation Acc: 61.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5/10], Step: [2701/3125], Validation Acc: 60.7\n",
      "Epoch: [5/10], Step: [2801/3125], Validation Acc: 61.8\n",
      "Epoch: [5/10], Step: [2901/3125], Validation Acc: 61.2\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 59.7\n",
      "Epoch: [5/10], Step: [3101/3125], Validation Acc: 61.4\n",
      "Epoch: [6/10], Step: [101/3125], Validation Acc: 62.4\n",
      "Epoch: [6/10], Step: [201/3125], Validation Acc: 60.4\n",
      "Epoch: [6/10], Step: [301/3125], Validation Acc: 61.5\n",
      "Epoch: [6/10], Step: [401/3125], Validation Acc: 61.7\n",
      "Epoch: [6/10], Step: [501/3125], Validation Acc: 61.7\n",
      "Epoch: [6/10], Step: [601/3125], Validation Acc: 62.6\n",
      "Epoch: [6/10], Step: [701/3125], Validation Acc: 59.8\n",
      "Epoch: [6/10], Step: [801/3125], Validation Acc: 61.0\n",
      "Epoch: [6/10], Step: [901/3125], Validation Acc: 60.0\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 60.7\n",
      "Epoch: [6/10], Step: [1101/3125], Validation Acc: 60.0\n",
      "Epoch: [6/10], Step: [1201/3125], Validation Acc: 60.4\n",
      "Epoch: [6/10], Step: [1301/3125], Validation Acc: 60.8\n",
      "Epoch: [6/10], Step: [1401/3125], Validation Acc: 61.2\n",
      "Epoch: [6/10], Step: [1501/3125], Validation Acc: 61.8\n",
      "Epoch: [6/10], Step: [1601/3125], Validation Acc: 61.4\n",
      "Epoch: [6/10], Step: [1701/3125], Validation Acc: 62.2\n",
      "Epoch: [6/10], Step: [1801/3125], Validation Acc: 61.3\n",
      "Epoch: [6/10], Step: [1901/3125], Validation Acc: 60.3\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 60.6\n",
      "Epoch: [6/10], Step: [2101/3125], Validation Acc: 60.9\n",
      "Epoch: [6/10], Step: [2201/3125], Validation Acc: 61.7\n",
      "Epoch: [6/10], Step: [2301/3125], Validation Acc: 60.4\n",
      "Epoch: [6/10], Step: [2401/3125], Validation Acc: 59.7\n",
      "Epoch: [6/10], Step: [2501/3125], Validation Acc: 61.7\n",
      "Epoch: [6/10], Step: [2601/3125], Validation Acc: 60.3\n",
      "Epoch: [6/10], Step: [2701/3125], Validation Acc: 60.3\n",
      "Epoch: [6/10], Step: [2801/3125], Validation Acc: 60.6\n",
      "Epoch: [6/10], Step: [2901/3125], Validation Acc: 60.9\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 60.6\n",
      "Epoch: [6/10], Step: [3101/3125], Validation Acc: 61.9\n",
      "Epoch: [7/10], Step: [101/3125], Validation Acc: 59.2\n",
      "Epoch: [7/10], Step: [201/3125], Validation Acc: 60.8\n",
      "Epoch: [7/10], Step: [301/3125], Validation Acc: 60.0\n",
      "Epoch: [7/10], Step: [401/3125], Validation Acc: 60.1\n",
      "Epoch: [7/10], Step: [501/3125], Validation Acc: 59.6\n",
      "Epoch: [7/10], Step: [601/3125], Validation Acc: 59.4\n",
      "Epoch: [7/10], Step: [701/3125], Validation Acc: 58.4\n",
      "Epoch: [7/10], Step: [801/3125], Validation Acc: 59.6\n",
      "Epoch: [7/10], Step: [901/3125], Validation Acc: 59.8\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 61.5\n",
      "Epoch: [7/10], Step: [1101/3125], Validation Acc: 60.1\n",
      "Epoch: [7/10], Step: [1201/3125], Validation Acc: 61.2\n",
      "Epoch: [7/10], Step: [1301/3125], Validation Acc: 59.4\n",
      "Epoch: [7/10], Step: [1401/3125], Validation Acc: 59.6\n",
      "Epoch: [7/10], Step: [1501/3125], Validation Acc: 59.5\n",
      "Epoch: [7/10], Step: [1601/3125], Validation Acc: 61.1\n",
      "Epoch: [7/10], Step: [1701/3125], Validation Acc: 61.3\n",
      "Epoch: [7/10], Step: [1801/3125], Validation Acc: 60.7\n",
      "Epoch: [7/10], Step: [1901/3125], Validation Acc: 60.8\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 62.0\n",
      "Epoch: [7/10], Step: [2101/3125], Validation Acc: 60.4\n",
      "Epoch: [7/10], Step: [2201/3125], Validation Acc: 60.8\n",
      "Epoch: [7/10], Step: [2301/3125], Validation Acc: 60.2\n",
      "Epoch: [7/10], Step: [2401/3125], Validation Acc: 60.6\n",
      "Epoch: [7/10], Step: [2501/3125], Validation Acc: 61.1\n",
      "Epoch: [7/10], Step: [2601/3125], Validation Acc: 60.8\n",
      "Epoch: [7/10], Step: [2701/3125], Validation Acc: 61.4\n",
      "Epoch: [7/10], Step: [2801/3125], Validation Acc: 60.0\n",
      "Epoch: [7/10], Step: [2901/3125], Validation Acc: 61.7\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 59.9\n",
      "Epoch: [7/10], Step: [3101/3125], Validation Acc: 61.4\n",
      "Epoch: [8/10], Step: [101/3125], Validation Acc: 60.8\n",
      "Epoch: [8/10], Step: [201/3125], Validation Acc: 60.2\n",
      "Epoch: [8/10], Step: [301/3125], Validation Acc: 61.5\n",
      "Epoch: [8/10], Step: [401/3125], Validation Acc: 60.8\n",
      "Epoch: [8/10], Step: [501/3125], Validation Acc: 60.0\n",
      "Epoch: [8/10], Step: [601/3125], Validation Acc: 60.1\n",
      "Epoch: [8/10], Step: [701/3125], Validation Acc: 59.5\n",
      "Epoch: [8/10], Step: [801/3125], Validation Acc: 60.0\n",
      "Epoch: [8/10], Step: [901/3125], Validation Acc: 60.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-dfe00302f28f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-248-fde0c056d56e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0membed_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mhidden_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0membed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 176\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_classes = len(train_data['label_num'].unique())\n",
    "model = CNN(weights_matrix, hidden_size=200, num_layers=2, num_classes= num_classes, kernel_size= 3 ,stride = 5)\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_1, lengths_1, data_2, lengths_2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data_1, data_2)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "    loss_list.append(loss.data)\n",
    "    accuracy_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.0854),\n",
       " tensor(1.1486),\n",
       " tensor(1.1277),\n",
       " tensor(1.0347),\n",
       " tensor(1.0586),\n",
       " tensor(1.0631),\n",
       " tensor(0.9696),\n",
       " tensor(1.0109),\n",
       " tensor(0.9247),\n",
       " tensor(1.0744),\n",
       " tensor(0.9471),\n",
       " tensor(1.0133),\n",
       " tensor(0.9145),\n",
       " tensor(1.0260),\n",
       " tensor(1.0667),\n",
       " tensor(1.2077),\n",
       " tensor(0.9675),\n",
       " tensor(1.1202),\n",
       " tensor(0.9092),\n",
       " tensor(0.8133),\n",
       " tensor(0.9675),\n",
       " tensor(0.8215),\n",
       " tensor(0.9508),\n",
       " tensor(0.8404),\n",
       " tensor(0.7954),\n",
       " tensor(0.9242),\n",
       " tensor(0.8645),\n",
       " tensor(0.7255),\n",
       " tensor(0.7863),\n",
       " tensor(0.7590),\n",
       " tensor(0.7228),\n",
       " tensor(0.8705),\n",
       " tensor(0.7469),\n",
       " tensor(0.8872),\n",
       " tensor(0.8345),\n",
       " tensor(1.0622),\n",
       " tensor(0.8043),\n",
       " tensor(0.9650),\n",
       " tensor(0.8950),\n",
       " tensor(0.9589),\n",
       " tensor(0.9285),\n",
       " tensor(0.8080),\n",
       " tensor(0.9105),\n",
       " tensor(0.8406),\n",
       " tensor(1.1042),\n",
       " tensor(0.8177),\n",
       " tensor(0.8388),\n",
       " tensor(0.7125),\n",
       " tensor(0.9619),\n",
       " tensor(0.7619),\n",
       " tensor(0.8477),\n",
       " tensor(0.9057),\n",
       " tensor(0.7922),\n",
       " tensor(0.7365),\n",
       " tensor(0.9448),\n",
       " tensor(0.9679),\n",
       " tensor(0.9810),\n",
       " tensor(0.8054),\n",
       " tensor(0.9078),\n",
       " tensor(0.9296),\n",
       " tensor(0.9092),\n",
       " tensor(0.7429),\n",
       " tensor(0.7288),\n",
       " tensor(0.6350),\n",
       " tensor(0.7937),\n",
       " tensor(0.6732),\n",
       " tensor(0.7399),\n",
       " tensor(0.9143),\n",
       " tensor(0.8184),\n",
       " tensor(0.9838),\n",
       " tensor(0.9108),\n",
       " tensor(0.7075),\n",
       " tensor(0.7686),\n",
       " tensor(0.7451),\n",
       " tensor(0.8036),\n",
       " tensor(0.7552),\n",
       " tensor(0.7739),\n",
       " tensor(0.6788),\n",
       " tensor(0.9066),\n",
       " tensor(0.7102),\n",
       " tensor(0.8175),\n",
       " tensor(0.7416),\n",
       " tensor(0.7482),\n",
       " tensor(0.7968),\n",
       " tensor(0.8793),\n",
       " tensor(0.9567),\n",
       " tensor(0.6412),\n",
       " tensor(0.6811),\n",
       " tensor(0.9757),\n",
       " tensor(0.8870),\n",
       " tensor(0.9489),\n",
       " tensor(0.8316),\n",
       " tensor(0.8251),\n",
       " tensor(0.9422),\n",
       " tensor(0.7764),\n",
       " tensor(0.7267),\n",
       " tensor(0.7893),\n",
       " tensor(0.8612),\n",
       " tensor(0.5710),\n",
       " tensor(0.7589),\n",
       " tensor(0.6105),\n",
       " tensor(0.7963),\n",
       " tensor(0.7217),\n",
       " tensor(0.8613),\n",
       " tensor(0.8520),\n",
       " tensor(0.8797),\n",
       " tensor(0.9077),\n",
       " tensor(1.0697),\n",
       " tensor(0.7136),\n",
       " tensor(0.7224),\n",
       " tensor(0.5931),\n",
       " tensor(0.6930),\n",
       " tensor(0.8587),\n",
       " tensor(0.7618),\n",
       " tensor(0.6328),\n",
       " tensor(0.6348),\n",
       " tensor(0.9005),\n",
       " tensor(0.9263),\n",
       " tensor(0.7304),\n",
       " tensor(0.7438),\n",
       " tensor(0.9483),\n",
       " tensor(0.7513),\n",
       " tensor(0.9279),\n",
       " tensor(0.8216),\n",
       " tensor(0.5924),\n",
       " tensor(0.7045),\n",
       " tensor(0.6789),\n",
       " tensor(0.7750),\n",
       " tensor(0.7908),\n",
       " tensor(0.6365),\n",
       " tensor(1.0023),\n",
       " tensor(0.6419),\n",
       " tensor(0.7371),\n",
       " tensor(0.7704),\n",
       " tensor(0.6570),\n",
       " tensor(0.6892),\n",
       " tensor(0.6230),\n",
       " tensor(0.6814),\n",
       " tensor(0.8427),\n",
       " tensor(0.5387),\n",
       " tensor(0.7593),\n",
       " tensor(0.7376),\n",
       " tensor(0.7070),\n",
       " tensor(0.7580),\n",
       " tensor(0.7292),\n",
       " tensor(0.6313),\n",
       " tensor(0.7136),\n",
       " tensor(0.6012),\n",
       " tensor(0.5827),\n",
       " tensor(0.7322),\n",
       " tensor(0.4617),\n",
       " tensor(1.0110),\n",
       " tensor(0.7718),\n",
       " tensor(0.5933),\n",
       " tensor(0.6558),\n",
       " tensor(0.5967),\n",
       " tensor(0.6365),\n",
       " tensor(0.6490),\n",
       " tensor(0.5570),\n",
       " tensor(0.6912),\n",
       " tensor(0.7723),\n",
       " tensor(0.7180),\n",
       " tensor(0.8683),\n",
       " tensor(0.7215),\n",
       " tensor(0.5648),\n",
       " tensor(0.5860),\n",
       " tensor(0.7417),\n",
       " tensor(0.6215),\n",
       " tensor(0.7279),\n",
       " tensor(0.9041),\n",
       " tensor(0.6386),\n",
       " tensor(0.5784),\n",
       " tensor(0.6736),\n",
       " tensor(0.4676),\n",
       " tensor(0.8230),\n",
       " tensor(0.5783),\n",
       " tensor(0.6145),\n",
       " tensor(0.7231),\n",
       " tensor(0.5729),\n",
       " tensor(0.7175),\n",
       " tensor(0.4956),\n",
       " tensor(0.5932),\n",
       " tensor(0.6273),\n",
       " tensor(0.6979),\n",
       " tensor(0.8157),\n",
       " tensor(0.7962),\n",
       " tensor(0.5057),\n",
       " tensor(0.5919),\n",
       " tensor(0.4898),\n",
       " tensor(0.6496),\n",
       " tensor(0.4163),\n",
       " tensor(0.8625),\n",
       " tensor(0.5848),\n",
       " tensor(0.5099),\n",
       " tensor(0.8333),\n",
       " tensor(0.6136),\n",
       " tensor(0.6661),\n",
       " tensor(0.5549),\n",
       " tensor(0.6415),\n",
       " tensor(0.5895),\n",
       " tensor(0.4410),\n",
       " tensor(0.7731),\n",
       " tensor(0.7458),\n",
       " tensor(0.6652),\n",
       " tensor(0.7013),\n",
       " tensor(0.8345),\n",
       " tensor(0.5682),\n",
       " tensor(0.6718),\n",
       " tensor(0.4978),\n",
       " tensor(0.4585),\n",
       " tensor(0.5559),\n",
       " tensor(0.7644),\n",
       " tensor(0.4179),\n",
       " tensor(1.1746),\n",
       " tensor(0.5097),\n",
       " tensor(0.4772),\n",
       " tensor(0.8140),\n",
       " tensor(0.6743),\n",
       " tensor(0.6818),\n",
       " tensor(0.5101),\n",
       " tensor(0.5022),\n",
       " tensor(0.6264),\n",
       " tensor(0.7564),\n",
       " tensor(0.6451),\n",
       " tensor(0.4758),\n",
       " tensor(0.4771)]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
