{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import pickle as pkl\n",
    "\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 1: Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = os.getcwd() + '/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_file_path):\n",
    "    #read files from train and test folders separately\n",
    "    for folders in ['train','test']:\n",
    "        path = data_file_path + '/' + folders\n",
    "        pos = list()\n",
    "        for f in os.listdir(path + '/pos'):\n",
    "            if f.split('.')[1] == 'txt':\n",
    "                f_content = open(path + '/pos/' + f, 'r+')\n",
    "                pos.append(f_content.read())\n",
    "    \n",
    "        y_pos = np.ones(len(pos))  \n",
    "    \n",
    "        neg = list()\n",
    "        for m in os.listdir(path + '/neg'):\n",
    "            if m.split('.')[1] == 'txt':\n",
    "                m_content = open(path + '/neg/' + m, 'r+')\n",
    "                neg.append(m_content.read())\n",
    "        y_neg = np.zeros(len(neg))\n",
    "        \n",
    "        if folders == 'train':\n",
    "            train = pd.DataFrame({'reviews': pos,'y':y_pos}).append(pd.DataFrame({'reviews': neg,'y':y_neg}))\n",
    "        if folders == 'test':\n",
    "            test = pd.DataFrame({'reviews': pos,'y':y_pos}).append(pd.DataFrame({'reviews': neg,'y':y_neg}))\n",
    "        \n",
    "    # Split train data into actual train and validation sets\n",
    "    shuffle(train)\n",
    "    train_split = 20000\n",
    "    train_data = train[:train_split]\n",
    "    vali_data = train[train_split:]\n",
    "    \n",
    "    return train_data, vali_data, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, vali, test = read_data(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "print (\"Train dataset size is {}\".format(len(train)))\n",
    "print (\"Val dataset size is {}\".format(len(vali)))\n",
    "print (\"Test dataset size is {}\".format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 2: Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-gram tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "\n",
    "#use nltk to tokenize the dataset, which is faster\n",
    "#and implement n-grams all at one function\n",
    "def tokenize_dataset_ngram(dataset,n_gram):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for text in dataset:\n",
    "        #tokenize and remove punctuations\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if (token not in punctuations)]\n",
    "        \n",
    "        n_grams_tokens = ngrams(tokens,n_gram)\n",
    "        n_grams_tokens = [ ' '.join(grams) for grams in n_grams_tokens]\n",
    "        token_dataset.append(n_grams_tokens)\n",
    "        all_tokens += n_grams_tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#save all tokens in local for convenience\n",
    "# val set tokens\n",
    "print (\"Tokenizing validation data\")\n",
    "vali_data_tokens, _ = tokenize_dataset_ngram(vali['reviews'],1)\n",
    "pkl.dump(vali_data_tokens, open(\"token data/vali_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset_ngram(test['reviews'],1)\n",
    "pkl.dump(test_data_tokens, open(\"token data/test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset_ngram(train['reviews'],1)\n",
    "pkl.dump(train_data_tokens, open(\"token data/train_data_tokens.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"token data/all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4858141\n"
     ]
    }
   ],
   "source": [
    "#check dataset size before move on to next step\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(vali_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens,vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocab(all_train_tokens,max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 3040 ; token patient\n",
      "Token patient; token id 3040\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data,token2id):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens,token2id)\n",
    "vali_data_indices = token2index_dataset(vali_data_tokens,token2id)\n",
    "test_data_indices = token2index_dataset(test_data_tokens,token2id)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(vali_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step 3: PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MoviewReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def moviereview_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(batch_size,data,label):\n",
    "    dataset = MoviewReviewDataset(data, list(label))\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=moviereview_collate_func,\n",
    "                                           shuffle=True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_val = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        \n",
    "        \n",
    "        loss_val += loss.item() * len(data) / len(loader.dataset)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        \n",
    "        \n",
    "    return (100 * correct / total),loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(num_epochs,train_loader,model,optimizer,vali_loader):\n",
    "    train_loss_history = []\n",
    "    vali_loss_history = []\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            train_loss += loss.item() * len(data) / len(train_loader.dataset)\n",
    "    \n",
    "        vali_acc,vali_loss = test_model(vali_loader, model)\n",
    "        train_loss_history.append(train_loss)\n",
    "        vali_loss_history.append(vali_loss) \n",
    "\n",
    "        print('Epoch: [{}/{}],Validation Acc: {}'.format( \n",
    "                               epoch+1, num_epochs, vali_acc))\n",
    "        \n",
    "    return train_loss_history,vali_loss_history\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-433-5c59b382d5fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvali_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvali_data_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvali\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-6101ff7c38ca>\u001b[0m in \u001b[0;36mcreate_data_loader\u001b[0;34m(batch_size, data, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoviewReviewDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     data_loader = torch.utils.data.DataLoader(dataset=dataset, \n\u001b[1;32m      4\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                            \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmoviereview_collate_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-432-165eeb934de1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_list, target_list)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got list)"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = create_data_loader(BATCH_SIZE,train_data_indices,train['y'])\n",
    "vali_loader = create_data_loader(BATCH_SIZE,vali_data_indices,vali['y'])\n",
    "test_loader = create_data_loader(BATCH_SIZE,test_data_indices,test['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Without tunning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10],Validation Acc: 69.96\n",
      "Epoch: [2/10],Validation Acc: 77.0\n",
      "Epoch: [3/10],Validation Acc: 71.4\n",
      "Epoch: [4/10],Validation Acc: 80.82\n",
      "Epoch: [5/10],Validation Acc: 77.88\n",
      "Epoch: [6/10],Validation Acc: 83.62\n",
      "Epoch: [7/10],Validation Acc: 76.58\n",
      "Epoch: [8/10],Validation Acc: 77.18\n",
      "Epoch: [9/10],Validation Acc: 78.68\n",
      "Epoch: [10/10],Validation Acc: 69.6\n"
     ]
    }
   ],
   "source": [
    "train_loss,vali_loss = run_model(num_epochs,train_loader,model,optimizer,vali_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc (72.66, 2.3511073375701894)\n",
      "Test Acc (79.168, 2.2855968953704866)\n"
     ]
    }
   ],
   "source": [
    "print(\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {0}\".format(test_model(vali_loader, model)[0]))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xa712774e0>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss,label = 'training loss')\n",
    "plt.plot(vali_loss,label = 'validation loss')\n",
    "plt.title(\"training curve with default hyper-parameters\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tune Hyper-Parameters: learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10],Validation Acc: 83.28\n",
      "Epoch: [2/10],Validation Acc: 71.98\n",
      "Epoch: [3/10],Validation Acc: 77.94\n",
      "Epoch: [4/10],Validation Acc: 80.44\n",
      "Epoch: [5/10],Validation Acc: 73.58\n",
      "Epoch: [6/10],Validation Acc: 74.66\n",
      "Epoch: [7/10],Validation Acc: 83.04\n",
      "Epoch: [8/10],Validation Acc: 78.68\n",
      "Epoch: [9/10],Validation Acc: 77.52\n",
      "Epoch: [10/10],Validation Acc: 80.52\n",
      "learning rate:0.05\n",
      "Val Acc (80.52, 2.2721121147155765)\n",
      "Test Acc (79.824, 2.2797203964233375)\n",
      "Epoch: [1/10],Validation Acc: 81.88\n",
      "Epoch: [2/10],Validation Acc: 73.54\n",
      "Epoch: [3/10],Validation Acc: 57.98\n",
      "Epoch: [4/10],Validation Acc: 76.94\n",
      "Epoch: [5/10],Validation Acc: 73.68\n",
      "Epoch: [6/10],Validation Acc: 88.12\n",
      "Epoch: [7/10],Validation Acc: 56.4\n",
      "Epoch: [8/10],Validation Acc: 83.2\n",
      "Epoch: [9/10],Validation Acc: 77.72\n",
      "Epoch: [10/10],Validation Acc: 74.98\n",
      "learning rate:0.1\n",
      "Val Acc (74.98, 2.3301225555419913)\n",
      "Test Acc (80.212, 2.2767430051422104)\n",
      "Epoch: [1/10],Validation Acc: 92.96\n",
      "Epoch: [2/10],Validation Acc: 74.68\n",
      "Epoch: [3/10],Validation Acc: 86.34\n",
      "Epoch: [4/10],Validation Acc: 57.22\n",
      "Epoch: [5/10],Validation Acc: 80.02\n",
      "Epoch: [6/10],Validation Acc: 68.9\n",
      "Epoch: [7/10],Validation Acc: 69.04\n",
      "Epoch: [8/10],Validation Acc: 88.44\n",
      "Epoch: [9/10],Validation Acc: 75.44\n",
      "Epoch: [10/10],Validation Acc: 79.84\n",
      "learning rate:0.5\n",
      "Val Acc (79.84, 2.279573922348023)\n",
      "Test Acc (80.98, 2.2682196627044666)\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "learning_rates = [0.05,0.1,0.5]\n",
    "num_epochs = 10\n",
    "\n",
    "loss_history = dict()\n",
    "accuracy_history = dict()\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"learning rate:{}\".format(lr))\n",
    "    train_loss,vali_loss = run_model(num_epochs,train_loader,model,optimizer,vali_loader)\n",
    "    print (\"Val Acc {0}\".format(test_model(vali_loader, model)))\n",
    "    print (\"Test Acc {}\".format(test_model(test_loader, model)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_result = pd.DataFrame({'Learning_Rate':[0.01,0.05,0.10,0.50],'Optimizer':'Adam','Embedding_Size':100,'Vocabulary_Size':10000,'Max_Validation_Accuracy':[83.62,83.28,88.12,92.96],'Last_Validation_Accuracy':[72.66,80.52,74.98,79.84]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning_Rate</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Embedding_Size</th>\n",
       "      <th>Vocabulary_Size</th>\n",
       "      <th>Max_Validation_Accuracy</th>\n",
       "      <th>Last_Validation_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>83.62</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>83.28</td>\n",
       "      <td>80.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>88.12</td>\n",
       "      <td>74.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>92.96</td>\n",
       "      <td>79.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning_Rate Optimizer  Embedding_Size  Vocabulary_Size  \\\n",
       "0           0.01      Adam             100            10000   \n",
       "1           0.05      Adam             100            10000   \n",
       "2           0.10      Adam             100            10000   \n",
       "3           0.50      Adam             100            10000   \n",
       "\n",
       "   Max_Validation_Accuracy  Last_Validation_Accuracy  \n",
       "0                    83.62                     72.66  \n",
       "1                    83.28                     80.52  \n",
       "2                    88.12                     74.98  \n",
       "3                    92.96                     79.84  "
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test accuracy:80.98\n",
      "Associated learning rate:0.5\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = max(learning_rate_result['Final Test Accuracy'])\n",
    "best_learning_rate = learning_rate_result[\n",
    "    learning_rate_result['Final Test Accuracy'] == best_accuracy]['learning_rate'].values[0]\n",
    "print(\"Best test accuracy:{}\".format(best_accuracy))\n",
    "print(\"Associated learning rate:{}\".format(best_learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in learning_rates:\n",
    "    plt.plot(loss_history[lr]['train_loss'],label = 'training loss (learning_rate={})'.format(lr))\n",
    "    plt.plot(loss_history[lr]['vali_loss'],label = 'validation loss (learning_rate={})'.format(lr))\n",
    "\n",
    "plt.title(\"training curve with different learning rates\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tune Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer:Adam\n",
      "Epoch: [1/10],Validation Acc: 63.6\n",
      "Epoch: [2/10],Validation Acc: 76.2\n",
      "Epoch: [3/10],Validation Acc: 83.08\n",
      "Epoch: [4/10],Validation Acc: 64.94\n",
      "Epoch: [5/10],Validation Acc: 83.78\n",
      "Epoch: [6/10],Validation Acc: 74.8\n",
      "Epoch: [7/10],Validation Acc: 76.22\n",
      "Epoch: [8/10],Validation Acc: 77.96\n",
      "Epoch: [9/10],Validation Acc: 80.64\n",
      "Epoch: [10/10],Validation Acc: 79.06\n",
      "Val Acc 79.06\n",
      "Test Acc 81.216\n",
      "optimizer:SGD\n",
      "Epoch: [1/10],Validation Acc: 58.76\n",
      "Epoch: [2/10],Validation Acc: 48.04\n",
      "Epoch: [3/10],Validation Acc: 48.68\n",
      "Epoch: [4/10],Validation Acc: 57.56\n",
      "Epoch: [5/10],Validation Acc: 62.38\n",
      "Epoch: [6/10],Validation Acc: 58.06\n",
      "Epoch: [7/10],Validation Acc: 56.26\n",
      "Epoch: [8/10],Validation Acc: 75.56\n",
      "Epoch: [9/10],Validation Acc: 68.62\n",
      "Epoch: [10/10],Validation Acc: 78.74\n",
      "Val Acc 78.74\n",
      "Test Acc 78.808\n"
     ]
    }
   ],
   "source": [
    "optimizer_list = ['Adam','SGD']\n",
    "num_epochs = 10\n",
    "emb_dim = 100\n",
    "learning_rate = best_learning_rate\n",
    "\n",
    "loss_history = dict()\n",
    "accuracy_history = dict()\n",
    "\n",
    "for opt in optimizer_list:\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    if opt == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if opt == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Optimizer:{}\".format(opt))\n",
    "    train_loss,vali_loss = run_model(num_epochs,train_loader,model,optimizer,vali_loader)\n",
    "    print (\"Val Acc {0}\".format(test_model(vali_loader, model)[0]))\n",
    "    print (\"Test Acc {}\".format(test_model(test_loader, model)[0]))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_result = pd.DataFrame({'Learning_Rate':learning_rate,'Optimizer':optimizer_list,'Embedding_Size':100,'Vocabulary_Size':10000,'Max_Validation_Accuracy':[83.08,78.74],'Last_Validation_Accuracy':[79.06,78.75]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning_Rate</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Embedding_Size</th>\n",
       "      <th>Vocabulary_Size</th>\n",
       "      <th>Max_Validation_Accuracy</th>\n",
       "      <th>Last_Validation_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>83.08</td>\n",
       "      <td>79.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>SGD</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>78.74</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning_Rate Optimizer  Embedding_Size  Vocabulary_Size  \\\n",
       "0            0.5      Adam             100            10000   \n",
       "1            0.5       SGD             100            10000   \n",
       "\n",
       "   Max_Validation_Accuracy  Last_Validation_Accuracy  \n",
       "0                    83.08                     79.06  \n",
       "1                    78.74                     78.75  "
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test accuracy:81.216\n",
      "Associated optimizer:Adam\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = max(optimizer_result['Final Test Accuracy'])\n",
    "best_optimizer = optimizer_result[\n",
    "    optimizer_result['Final Test Accuracy'] == best_accuracy]['Optimizer'].values[0]\n",
    "print(\"Best test accuracy:{}\".format(best_accuracy))\n",
    "print(\"Associated optimizer:{}\".format(best_optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tune Embedding-Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_Size:50\n",
      "Epoch: [1/10],Validation Acc: 86.78\n",
      "Epoch: [2/10],Validation Acc: 80.36\n",
      "Epoch: [3/10],Validation Acc: 60.62\n",
      "Epoch: [4/10],Validation Acc: 74.18\n",
      "Epoch: [5/10],Validation Acc: 69.22\n",
      "Epoch: [6/10],Validation Acc: 71.76\n",
      "Epoch: [7/10],Validation Acc: 81.46\n",
      "Epoch: [8/10],Validation Acc: 66.72\n",
      "Epoch: [9/10],Validation Acc: 77.18\n",
      "Epoch: [10/10],Validation Acc: 80.84\n",
      "Val Acc 80.84\n",
      "Test Acc 80.672\n",
      "\n",
      "\n",
      "Embedding_Size:100\n",
      "Epoch: [1/10],Validation Acc: 59.78\n",
      "Epoch: [2/10],Validation Acc: 78.22\n",
      "Epoch: [3/10],Validation Acc: 64.52\n",
      "Epoch: [4/10],Validation Acc: 76.72\n",
      "Epoch: [5/10],Validation Acc: 75.4\n",
      "Epoch: [6/10],Validation Acc: 84.48\n",
      "Epoch: [7/10],Validation Acc: 83.6\n",
      "Epoch: [8/10],Validation Acc: 80.58\n",
      "Epoch: [9/10],Validation Acc: 83.06\n",
      "Epoch: [10/10],Validation Acc: 73.2\n",
      "Val Acc 73.2\n",
      "Test Acc 80.148\n",
      "\n",
      "\n",
      "Embedding_Size:150\n",
      "Epoch: [1/10],Validation Acc: 60.96\n",
      "Epoch: [2/10],Validation Acc: 83.48\n",
      "Epoch: [3/10],Validation Acc: 75.54\n",
      "Epoch: [4/10],Validation Acc: 84.06\n",
      "Epoch: [5/10],Validation Acc: 68.36\n",
      "Epoch: [6/10],Validation Acc: 57.26\n",
      "Epoch: [7/10],Validation Acc: 72.34\n",
      "Epoch: [8/10],Validation Acc: 69.86\n",
      "Epoch: [9/10],Validation Acc: 69.12\n",
      "Epoch: [10/10],Validation Acc: 82.06\n",
      "Val Acc 82.06\n",
      "Test Acc 80.888\n",
      "\n",
      "\n",
      "Embedding_Size:200\n",
      "Epoch: [1/10],Validation Acc: 71.18\n",
      "Epoch: [2/10],Validation Acc: 67.1\n",
      "Epoch: [3/10],Validation Acc: 83.7\n",
      "Epoch: [4/10],Validation Acc: 83.76\n",
      "Epoch: [5/10],Validation Acc: 81.1\n",
      "Epoch: [6/10],Validation Acc: 73.98\n",
      "Epoch: [7/10],Validation Acc: 70.66\n",
      "Epoch: [8/10],Validation Acc: 80.38\n",
      "Epoch: [9/10],Validation Acc: 79.8\n",
      "Epoch: [10/10],Validation Acc: 73.4\n",
      "Val Acc 73.4\n",
      "Test Acc 80.184\n",
      "\n",
      "\n",
      "Embedding_Size:300\n",
      "Epoch: [1/10],Validation Acc: 70.06\n",
      "Epoch: [2/10],Validation Acc: 60.08\n",
      "Epoch: [3/10],Validation Acc: 81.68\n",
      "Epoch: [4/10],Validation Acc: 80.36\n",
      "Epoch: [5/10],Validation Acc: 80.16\n",
      "Epoch: [6/10],Validation Acc: 83.08\n",
      "Epoch: [7/10],Validation Acc: 80.38\n",
      "Epoch: [8/10],Validation Acc: 82.44\n",
      "Epoch: [9/10],Validation Acc: 64.5\n",
      "Epoch: [10/10],Validation Acc: 79.08\n",
      "Val Acc 79.08\n",
      "Test Acc 81.104\n",
      "\n",
      "\n",
      "Embedding_Size:500\n",
      "Epoch: [1/10],Validation Acc: 77.8\n",
      "Epoch: [2/10],Validation Acc: 72.26\n",
      "Epoch: [3/10],Validation Acc: 86.8\n",
      "Epoch: [4/10],Validation Acc: 68.68\n",
      "Epoch: [5/10],Validation Acc: 78.66\n",
      "Epoch: [6/10],Validation Acc: 87.66\n",
      "Epoch: [7/10],Validation Acc: 89.9\n",
      "Epoch: [8/10],Validation Acc: 65.42\n",
      "Epoch: [9/10],Validation Acc: 83.7\n",
      "Epoch: [10/10],Validation Acc: 82.48\n",
      "Val Acc 82.48\n",
      "Test Acc 80.688\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "emb_dim_list = [50,100,150,200,300,500]\n",
    "learning_rate = best_learning_rate\n",
    "\n",
    "loss_history = dict()\n",
    "accuracy_history = dict()\n",
    "\n",
    "for emb_dim in emb_dim_list:\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Embedding_Size:{}\".format(emb_dim))\n",
    "    train_loss,vali_loss = run_model(num_epochs,train_loader,model,optimizer,vali_loader)\n",
    "    print (\"Val Acc {0}\".format(test_model(vali_loader, model)[0]))\n",
    "    print (\"Test Acc {}\".format(test_model(test_loader, model)[0]))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size_result = pd.DataFrame({'Learning_Rate':learning_rate,'Optimizer':'Adam','Embedding_Size':emb_dim_list,'Vocabulary_Size':10000,'Max_Validation_Accuracy':[86.78,84.48,84.06,83.76,83.08,89.9],'Last_Validation_Accuracy':[80.84,73.2,82.06,73.4,79.08,82.48]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning_Rate</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Embedding_Size</th>\n",
       "      <th>Vocabulary_Size</th>\n",
       "      <th>Max_Validation_Accuracy</th>\n",
       "      <th>Last_Validation_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>50</td>\n",
       "      <td>10000</td>\n",
       "      <td>86.78</td>\n",
       "      <td>80.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>84.48</td>\n",
       "      <td>73.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>150</td>\n",
       "      <td>10000</td>\n",
       "      <td>84.06</td>\n",
       "      <td>82.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>200</td>\n",
       "      <td>10000</td>\n",
       "      <td>83.76</td>\n",
       "      <td>73.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>300</td>\n",
       "      <td>10000</td>\n",
       "      <td>83.08</td>\n",
       "      <td>79.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>Adam</td>\n",
       "      <td>500</td>\n",
       "      <td>10000</td>\n",
       "      <td>89.90</td>\n",
       "      <td>82.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning_Rate Optimizer  Embedding_Size  Vocabulary_Size  \\\n",
       "0            0.5      Adam              50            10000   \n",
       "1            0.5      Adam             100            10000   \n",
       "2            0.5      Adam             150            10000   \n",
       "3            0.5      Adam             200            10000   \n",
       "4            0.5      Adam             300            10000   \n",
       "5            0.5      Adam             500            10000   \n",
       "\n",
       "   Max_Validation_Accuracy  Last_Validation_Accuracy  \n",
       "0                    86.78                     80.84  \n",
       "1                    84.48                     73.20  \n",
       "2                    84.06                     82.06  \n",
       "3                    83.76                     73.40  \n",
       "4                    83.08                     79.08  \n",
       "5                    89.90                     82.48  "
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test accuracy:81.104\n",
      "Associated embedding_size:300\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = max(embedding_size_result['Final Test Accuracy'])\n",
    "best_embedding_size = embedding_size_result[\n",
    "    embedding_size_result['Final Test Accuracy'] == best_accuracy]['Embedding_Size'].values[0]\n",
    "print(\"Best test accuracy:{}\".format(best_accuracy))\n",
    "print(\"Associated embedding_size:{}\".format(best_embedding_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Building n-grams and re-train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data (bigram)\n",
      "Tokenizing test data (bigram)\n",
      "Tokenizing train data (bigram)\n"
     ]
    }
   ],
   "source": [
    "#save all tokens in local for convenience\n",
    "# val set tokens\n",
    "print (\"Tokenizing validation data (bigram)\")\n",
    "vali_data_tokens_bigram, _ = tokenize_dataset_ngram(vali['reviews'],2)\n",
    "pkl.dump(vali_data_tokens_bigram, open(\"token data/vali_data_tokens_bigram.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data (bigram)\")\n",
    "test_data_tokens_bigram, _ = tokenize_dataset_ngram(test['reviews'],2)\n",
    "pkl.dump(test_data_tokens_bigram, open(\"token data/test_data_tokens_bigram.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data (bigram)\")\n",
    "train_data_tokens_bigram, all_train_tokens_bigram = tokenize_dataset_ngram(train['reviews'],2)\n",
    "pkl.dump(train_data_tokens_bigram, open(\"token data/train_data_tokens_bigram.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_bigram, open(\"token data/all_train_tokens_bigram.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset (bigram) size is 20000\n",
      "Val dataset(bigram) size is 5000\n",
      "Test dataset(bigram) size is 25000\n",
      "Total number of tokens in train dataset is 4838141\n"
     ]
    }
   ],
   "source": [
    "# double checking dataset size\n",
    "print (\"Train dataset (bigram) size is {}\".format(len(train_data_tokens_bigram)))\n",
    "print (\"Val dataset(bigram) size is {}\".format(len(vali_data_tokens_bigram)))\n",
    "print (\"Test dataset(bigram) size is {}\".format(len(test_data_tokens_bigram)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_bigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_bigram, id2token_bigram = build_vocab(all_train_tokens_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 9328 ; token and intelligent\n",
      "Token and intelligent; token id 9328\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "random_token_id = random.randint(0, len(id2token_bigram)-1)\n",
    "random_token = id2token_bigram[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token_bigram[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id_bigram[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Vali dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_bigram = token2index_dataset(train_data_tokens_bigram,token2id_bigram)\n",
    "vali_data_indices_bigram = token2index_dataset(vali_data_tokens_bigram,token2id_bigram)\n",
    "test_data_indices_bigram = token2index_dataset(test_data_tokens_bigram,token2id_bigram)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_bigram)))\n",
    "print (\"Vali dataset size is {}\".format(len(vali_data_indices_bigram)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_bigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-train the model with new data\n",
    "BATCH_SIZE = 32\n",
    "bigram_train_loader = create_data_loader(BATCH_SIZE,train_data_indices_bigram,train['y'])\n",
    "bigram_vali_loader = create_data_loader(BATCH_SIZE,vali_data_indices_bigram,vali['y'])\n",
    "bigram_test_loader = create_data_loader(BATCH_SIZE,test_data_indices_bigram,test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token_bigram), emb_dim)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10],Validation Acc: 73.86\n",
      "Epoch: [2/10],Validation Acc: 74.8\n",
      "Epoch: [3/10],Validation Acc: 73.3\n",
      "Epoch: [4/10],Validation Acc: 70.9\n",
      "Epoch: [5/10],Validation Acc: 73.56\n",
      "Epoch: [6/10],Validation Acc: 70.5\n",
      "Epoch: [7/10],Validation Acc: 74.58\n",
      "Epoch: [8/10],Validation Acc: 70.3\n",
      "Epoch: [9/10],Validation Acc: 72.98\n",
      "Epoch: [10/10],Validation Acc: 75.4\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "vali_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (data, lengths, labels) in enumerate(bigram_train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    vali_acc,vali_loss = test_model(bigram_vali_loader, model)\n",
    "    train_loss_history.append(running_loss)\n",
    "    vali_loss_history.append(vali_loss)\n",
    "    \n",
    "    print('Epoch: [{}/{}],Validation Acc: {}'.format( \n",
    "                               epoch+1, num_epochs, vali_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 75.4\n",
      "Test Acc 77.948\n"
     ]
    }
   ],
   "source": [
    "print(\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {0}\".format(test_model(bigram_vali_loader, model)[0]))\n",
    "print (\"Test Acc {}\".format(test_model(bigram_test_loader, model)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Training on Bi-gram BOW Model\\n(lr = 0.01; embedding_size = 100; vocabulary_size = 10000)')"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(train_loss_history,label='trainning loss')\n",
    "#plt.plot(vali_loss_history,label='validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Training on Bi-gram BOW Model\\n(lr = 0.01; embedding_size = 100; vocabulary_size = 10000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Try tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data (trigram)\n",
      "Tokenizing test data (trigram)\n",
      "Tokenizing train data (trigram)\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing validation data (trigram)\")\n",
    "vali_data_tokens_trigram, _ = tokenize_dataset_ngram(vali['reviews'],3)\n",
    "pkl.dump(vali_data_tokens_bigram, open(\"token data/vali_data_tokens_trigram.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data (trigram)\")\n",
    "test_data_tokens_trigram, _ = tokenize_dataset_ngram(test['reviews'],3)\n",
    "pkl.dump(test_data_tokens_bigram, open(\"token data/test_data_tokens_trigram.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data (trigram)\")\n",
    "train_data_tokens_trigram, all_train_tokens_trigram = tokenize_dataset_ngram(train['reviews'],3)\n",
    "pkl.dump(train_data_tokens_bigram, open(\"token data/train_data_tokens_trigram.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_bigram, open(\"token data/all_train_tokens_trigram.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset (trigram) size is 20000\n",
      "Val dataset(trigram) size is 5000\n",
      "Test dataset(trigram) size is 25000\n",
      "Total number of tokens in train dataset is 4818141\n"
     ]
    }
   ],
   "source": [
    "# double checking dataset size\n",
    "print (\"Train dataset (trigram) size is {}\".format(len(train_data_tokens_trigram)))\n",
    "print (\"Val dataset(trigram) size is {}\".format(len(vali_data_tokens_trigram)))\n",
    "print (\"Test dataset(trigram) size is {}\".format(len(test_data_tokens_trigram)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_trigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_trigram, id2token_trigram = build_vocab(all_train_tokens_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Vali dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "train_data_indices_trigram = token2index_dataset(train_data_tokens_trigram,token2id_trigram)\n",
    "vali_data_indices_trigram = token2index_dataset(vali_data_tokens_trigram,token2id_trigram)\n",
    "test_data_indices_trigram = token2index_dataset(test_data_tokens_trigram,token2id_trigram)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices_trigram)))\n",
    "print (\"Vali dataset size is {}\".format(len(vali_data_indices_trigram)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices_trigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-train the model with new data\n",
    "BATCH_SIZE = 32\n",
    "trigram_train_loader = create_data_loader(BATCH_SIZE,train_data_indices_trigram,train['y'])\n",
    "trigram_vali_loader = create_data_loader(BATCH_SIZE,vali_data_indices_trigram,vali['y'])\n",
    "trigram_test_loader = create_data_loader(BATCH_SIZE,test_data_indices_trigram,test['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token_trigram), emb_dim)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(dataloader):\n",
    "    model.train()\n",
    "    loss_val = 0\n",
    "    for i, (data, lengths, labels) in enumerate(dataloader):\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accurately compute loss, because of different batch size\n",
    "        loss_val += loss.item() * len(data) / len(dataloader.dataset)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(dataloader):\n",
    "    model.eval()\n",
    "    loss_val = 0\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for i, (data, lengths, labels) in enumerate(dataloader):\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss_val += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10],Validation Acc: 62.54\n",
      "Epoch: [2/10],Validation Acc: 71.44\n",
      "Epoch: [3/10],Validation Acc: 71.0\n",
      "Epoch: [4/10],Validation Acc: 68.18\n",
      "Epoch: [5/10],Validation Acc: 66.88\n",
      "Epoch: [6/10],Validation Acc: 64.74\n",
      "Epoch: [7/10],Validation Acc: 69.6\n",
      "Epoch: [8/10],Validation Acc: 64.58\n",
      "Epoch: [9/10],Validation Acc: 68.26\n",
      "Epoch: [10/10],Validation Acc: 66.64\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(trigram_train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    vali_acc,vali_loss = test_model(trigram_vali_loader, model)\n",
    "    \n",
    "    print('Epoch: [{}/{}],Validation Acc: {}'.format( \n",
    "                               epoch+1, num_epochs, vali_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 66.64\n",
      "Test Acc 72.252\n"
     ]
    }
   ],
   "source": [
    "print(\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {0}\".format(test_model(trigram_vali_loader, model)[0]))\n",
    "print (\"Test Acc {}\".format(test_model(trigram_test_loader, model)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tune different vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_default_setting(id2token,train_loader,vali_loader):\n",
    "    emb_dim = 100\n",
    "    model = BagOfWords(len(id2token), emb_dim)\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 10 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss,vali_loss = run_model(num_epochs,train_loader,model,optimizer,vali_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:5000\n",
      "Epoch: [1/10],Validation Acc: 76.2\n",
      "Epoch: [2/10],Validation Acc: 76.44\n",
      "Epoch: [3/10],Validation Acc: 83.56\n",
      "Epoch: [4/10],Validation Acc: 75.04\n",
      "Epoch: [5/10],Validation Acc: 78.9\n",
      "Epoch: [6/10],Validation Acc: 79.6\n",
      "Epoch: [7/10],Validation Acc: 79.62\n",
      "Epoch: [8/10],Validation Acc: 79.22\n",
      "Epoch: [9/10],Validation Acc: 81.52\n",
      "Epoch: [10/10],Validation Acc: 78.12\n",
      "vocabulary size:10000\n",
      "Epoch: [1/10],Validation Acc: 85.14\n",
      "Epoch: [2/10],Validation Acc: 78.1\n",
      "Epoch: [3/10],Validation Acc: 80.36\n",
      "Epoch: [4/10],Validation Acc: 74.38\n",
      "Epoch: [5/10],Validation Acc: 74.58\n",
      "Epoch: [6/10],Validation Acc: 80.86\n",
      "Epoch: [7/10],Validation Acc: 76.36\n",
      "Epoch: [8/10],Validation Acc: 79.08\n",
      "Epoch: [9/10],Validation Acc: 75.54\n",
      "Epoch: [10/10],Validation Acc: 78.16\n",
      "vocabulary size:15000\n",
      "Epoch: [1/10],Validation Acc: 74.92\n",
      "Epoch: [2/10],Validation Acc: 79.36\n",
      "Epoch: [3/10],Validation Acc: 82.36\n",
      "Epoch: [4/10],Validation Acc: 79.4\n",
      "Epoch: [5/10],Validation Acc: 82.74\n",
      "Epoch: [6/10],Validation Acc: 80.38\n",
      "Epoch: [7/10],Validation Acc: 76.64\n",
      "Epoch: [8/10],Validation Acc: 74.44\n",
      "Epoch: [9/10],Validation Acc: 72.44\n",
      "Epoch: [10/10],Validation Acc: 82.82\n",
      "vocabulary size:20000\n",
      "Epoch: [1/10],Validation Acc: 81.94\n",
      "Epoch: [2/10],Validation Acc: 73.98\n",
      "Epoch: [3/10],Validation Acc: 72.34\n",
      "Epoch: [4/10],Validation Acc: 84.4\n",
      "Epoch: [5/10],Validation Acc: 76.16\n",
      "Epoch: [6/10],Validation Acc: 76.74\n",
      "Epoch: [7/10],Validation Acc: 80.0\n",
      "Epoch: [8/10],Validation Acc: 78.22\n",
      "Epoch: [9/10],Validation Acc: 76.56\n",
      "Epoch: [10/10],Validation Acc: 77.8\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size_lists = [5000,10000,15000,20000]\n",
    "for vocab_size in vocabulary_size_lists:\n",
    "    token2id, id2token = build_vocab(all_train_tokens,vocab_size)\n",
    "    \n",
    "    train_data_indices = token2index_dataset(train_data_tokens,token2id)\n",
    "    vali_data_indices = token2index_dataset(vali_data_tokens,token2id)\n",
    "    test_data_indices = token2index_dataset(test_data_tokens,token2id)\n",
    "    \n",
    "    if (len(train_data_indices) == 20000) & (len(vali_data_indices) == 5000) & (len(test_data_indices) == 25000):\n",
    "        train_loader = create_data_loader(BATCH_SIZE,train_data_indices,train['y'])\n",
    "        vali_loader = create_data_loader(BATCH_SIZE,vali_data_indices,vali['y'])\n",
    "        test_loader = create_data_loader(BATCH_SIZE,test_data_indices,test['y'])\n",
    "        \n",
    "        print(\"vocabulary size:{}\".format(vocab_size))\n",
    "        run_model_default_setting(id2token,train_loader,vali_loader)\n",
    "    else:\n",
    "        print(\"token to index step was wrong, please check!\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_result = pd.DataFrame({'Learning_Rate':0.01,'Optimizer':'Adam','Embedding_Size':100,'Vocabulary_Size':vocabulary_size_lists,'Max_Validation_Accuracy':[83.56,85.14,82.82,84.4],'Last_Validation_Accuracy':[78.12,78.16,82.82,77.8]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning_Rate</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Embedding_Size</th>\n",
       "      <th>Vocabulary_Size</th>\n",
       "      <th>Max_Validation_Accuracy</th>\n",
       "      <th>Last_Validation_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>5000</td>\n",
       "      <td>83.56</td>\n",
       "      <td>78.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>85.14</td>\n",
       "      <td>78.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>15000</td>\n",
       "      <td>82.82</td>\n",
       "      <td>82.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>20000</td>\n",
       "      <td>84.40</td>\n",
       "      <td>77.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning_Rate Optimizer  Embedding_Size  Vocabulary_Size  \\\n",
       "0           0.01      Adam             100             5000   \n",
       "1           0.01      Adam             100            10000   \n",
       "2           0.01      Adam             100            15000   \n",
       "3           0.01      Adam             100            20000   \n",
       "\n",
       "   Max_Validation_Accuracy  Last_Validation_Accuracy  \n",
       "0                    83.56                     78.12  \n",
       "1                    85.14                     78.16  \n",
       "2                    82.82                     82.82  \n",
       "3                    84.40                     77.80  "
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Try n-grams combined BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset_ngram_combined(dataset,n_gram):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for text in dataset:\n",
    "        #tokenize and remove punctuations\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [token.lower() for token in tokens if (token not in punctuations)]\n",
    "        combined_tokens = []\n",
    "        \n",
    "        for i in range(1,n_gram + 1):\n",
    "            n_grams_tokens = ngrams(tokens,i)\n",
    "            n_grams_tokens = [' '.join(grams) for grams in n_grams_tokens]\n",
    "            combined_tokens += n_grams_tokens\n",
    "            \n",
    "        token_dataset.append(combined_tokens)\n",
    "        all_tokens += combined_tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing validation data (trigram)\n",
      "Tokenizing test data (trigram)\n",
      "Tokenizing train data (trigram)\n"
     ]
    }
   ],
   "source": [
    "print (\"Tokenizing validation data (combined)\")\n",
    "vali_data_tokens_comb, _ = tokenize_dataset_ngram_combined(vali['reviews'],2)\n",
    "pkl.dump(vali_data_tokens_comb, open(\"token data/vali_data_tokens_combined.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data (combined)\")\n",
    "test_data_tokens_comb, _ = tokenize_dataset_ngram_combined(test['reviews'],2)\n",
    "pkl.dump(test_data_tokens_comb, open(\"token data/test_data_tokens_combined.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data (combined)\")\n",
    "train_data_tokens_comb, all_train_tokens_comb = tokenize_dataset_ngram_combined(train['reviews'],2)\n",
    "pkl.dump(train_data_tokens_comb, open(\"token data/train_data_tokens_combined.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_comb, open(\"token data/all_train_tokens_combined.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset (combined) size is 20000\n",
      "Val dataset(combined) size is 5000\n",
      "Test dataset(combined) size is 25000\n",
      "Total number of tokens in train dataset is 9696282\n"
     ]
    }
   ],
   "source": [
    "# double checking dataset size\n",
    "print (\"Train dataset (combined) size is {}\".format(len(train_data_tokens_comb)))\n",
    "print (\"Val dataset(combined) size is {}\".format(len(vali_data_tokens_comb)))\n",
    "print (\"Test dataset(combined) size is {}\".format(len(test_data_tokens_comb)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens_comb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id_comb, id2token_comb = build_vocab(all_train_tokens_comb,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indices_comb = token2index_dataset(train_data_tokens_comb,token2id_comb)\n",
    "vali_data_indices_comb = token2index_dataset(vali_data_tokens_comb,token2id_comb)\n",
    "test_data_indices_comb = token2index_dataset(test_data_tokens_comb,token2id_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10],Validation Acc: 76.86\n",
      "Epoch: [2/10],Validation Acc: 82.84\n",
      "Epoch: [3/10],Validation Acc: 74.22\n",
      "Epoch: [4/10],Validation Acc: 74.94\n",
      "Epoch: [5/10],Validation Acc: 79.46\n",
      "Epoch: [6/10],Validation Acc: 75.52\n",
      "Epoch: [7/10],Validation Acc: 77.76\n",
      "Epoch: [8/10],Validation Acc: 78.02\n",
      "Epoch: [9/10],Validation Acc: 76.18\n",
      "Epoch: [10/10],Validation Acc: 77.96\n",
      "After running 10 epochs\n",
      "Val Acc 77.96\n"
     ]
    }
   ],
   "source": [
    "if (len(train_data_indices_comb) == 20000) & (len(vali_data_indices_comb) == 5000) & (len(test_data_indices_comb) == 25000):\n",
    "    comb_train_loader = create_data_loader(BATCH_SIZE,train_data_indices_comb,train['y'])\n",
    "    comb_vali_loader = create_data_loader(BATCH_SIZE,vali_data_indices_comb,vali['y'])\n",
    "    comb_test_loader = create_data_loader(BATCH_SIZE,test_data_indices_comb,test['y'])\n",
    "\n",
    "    emb_dim = 100\n",
    "    model = BagOfWords(len(id2token_comb), emb_dim)\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 10 # number epoch to train\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    run_model(num_epochs,comb_train_loader,model,optimizer,comb_vali_loader)\n",
    "    print(\"After running 10 epochs\")\n",
    "    print (\"Val Acc {0}\".format(test_model(comb_vali_loader, model)[0]))\n",
    "else:\n",
    "    print(\"token to index step was wrong, please check!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams_result = pd.DataFrame({'n-gram':['1_gram','bi-gram','tri_gram','1+2_grams'],'Learning_Rate':0.01,'Optimizer':'Adam','Embedding_Size':100,'Vocabulary_Size':10000,'Max_Validation_Accuracy':[83.62,75.4,71.44,82.84],'Last_Validation_Accuracy':[72.66,75.4,66.64,77.96]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n-gram</th>\n",
       "      <th>Learning_Rate</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Embedding_Size</th>\n",
       "      <th>Vocabulary_Size</th>\n",
       "      <th>Max_Validation_Accuracy</th>\n",
       "      <th>Last_Validation_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_gram</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>83.62</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bi-gram</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>75.40</td>\n",
       "      <td>75.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tri_gram</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>71.44</td>\n",
       "      <td>66.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1+2_grams</td>\n",
       "      <td>0.01</td>\n",
       "      <td>Adam</td>\n",
       "      <td>100</td>\n",
       "      <td>10000</td>\n",
       "      <td>82.84</td>\n",
       "      <td>77.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      n-gram  Learning_Rate Optimizer  Embedding_Size  Vocabulary_Size  \\\n",
       "0     1_gram           0.01      Adam             100            10000   \n",
       "1    bi-gram           0.01      Adam             100            10000   \n",
       "2   tri_gram           0.01      Adam             100            10000   \n",
       "3  1+2_grams           0.01      Adam             100            10000   \n",
       "\n",
       "   Max_Validation_Accuracy  Last_Validation_Accuracy  \n",
       "0                    83.62                     72.66  \n",
       "1                    75.40                     75.40  \n",
       "2                    71.44                     66.64  \n",
       "3                    82.84                     77.96  "
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10],Training Loss: 258.72273148596287,Validation Loss:72.68006438016891\n",
      "Epoch: [2/10],Training Loss: 118.35792031139135,Validation Loss:64.18999163806438\n",
      "Epoch: [3/10],Training Loss: 66.72399761527777,Validation Loss:109.0859994739294\n",
      "Epoch: [4/10],Training Loss: 35.44490148127079,Validation Loss:111.71872924268246\n",
      "Epoch: [5/10],Training Loss: 19.480756975710392,Validation Loss:173.04688522219658\n",
      "Epoch: [6/10],Training Loss: 8.541881404817104,Validation Loss:217.13850861787796\n",
      "Epoch: [7/10],Training Loss: 4.082879945635796,Validation Loss:203.91680997610092\n",
      "Epoch: [8/10],Training Loss: 1.3065952956676483,Validation Loss:234.37704062461853\n",
      "Epoch: [9/10],Training Loss: 0.4623711556196213,Validation Loss:223.4012978374958\n",
      "Epoch: [10/10],Training Loss: 0.2628939300775528,Validation Loss:249.13265632092953\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    vali_loss = do_eval(vali_loader)\n",
    "\n",
    "    \n",
    "    print('Epoch: [{}/{}],Training Loss: {},Validation Loss:{}'.format( \n",
    "                               epoch+1, num_epochs, running_loss,vali_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
